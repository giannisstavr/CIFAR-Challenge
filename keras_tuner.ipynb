{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 06:42:30.546042: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-29 06:42:30.554642: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743230550.564345  106125 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743230550.567049  106125 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-29 06:42:30.578459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1743230554.500743  106125 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import applications, utils, layers ,callbacks \n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth for each GPU to true\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "def mixup_data(x, y, alpha):\n",
    "    \"\"\"\n",
    "    Returns mixed inputs and mixed targets.\n",
    "    \n",
    "    Args:\n",
    "        x: Batch of input images, shape (batch_size, height, width, channels)\n",
    "        y: Batch of one-hot encoded labels, shape (batch_size, num_classes)\n",
    "        alpha: Hyperparameter for the Beta distribution (default 0.2)\n",
    "    \n",
    "    Returns:\n",
    "        x_mix: Mixed inputs\n",
    "        y_mix: Mixed labels\n",
    "        lam: The mixing coefficient\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    # Shuffle the batch indices\n",
    "    indices = tf.random.shuffle(tf.range(batch_size))\n",
    "    x_shuffled = tf.gather(x, indices)\n",
    "    y_shuffled = tf.gather(y, indices)\n",
    "    \n",
    "    # Create mixed inputs and targets\n",
    "    x_mix = lam * x + (1 - lam) * x_shuffled\n",
    "    y_mix = lam * y + (1 - lam) * y_shuffled\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "\n",
    "def mixup_generator(x, y, batch_size, alpha):\n",
    "    \"\"\"\n",
    "    A generator that yields mixup-augmented batches.\n",
    "    \n",
    "    Args:\n",
    "        x: Training images as a NumPy array.\n",
    "        y: One-hot encoded labels as a NumPy array.\n",
    "        batch_size: Batch size.\n",
    "        alpha: Mixup hyperparameter.\n",
    "    \n",
    "    Yields:\n",
    "        A tuple (x_mix, y_mix) for training.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            x_batch = x[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            x_mix, y_mix, _ = mixup_data(x_batch, y_batch, alpha)\n",
    "            yield x_mix, y_mix\n",
    "\n",
    "# Load CIFAR-100 data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode=\"fine\")\n",
    "\n",
    "# For ResNet preprocessing, images are expected in range [0, 255]\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Apply ResNet-specific preprocessing (this function converts RGB to BGR and zero-centers using ImageNet means)\n",
    "x_train = applications.resnet.preprocess_input(x_train)\n",
    "x_test = applications.resnet.preprocess_input(x_test)\n",
    "\n",
    "# Convert labels to one-hot encoding (100 classes)\n",
    "y_train = utils.to_categorical(y_train, 100)\n",
    "y_test = utils.to_categorical(y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "batch_size = 32\n",
    "alpha = 0.5 #augmenation intensity\n",
    "steps_per_epoch = x_train.shape[0] // batch_size\n",
    "\n",
    "# Create the mixup generator\n",
    "train_gen = mixup_generator(x_train, y_train, batch_size, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple ResNet50 model (using weights=None allows custom input shape, here 32x32 for CIFAR-100)\n",
    "base_model = applications.ResNet50(\n",
    "    input_shape=(224,224, 3), weights=\"imagenet\", include_top= False #pooling doesn't work\n",
    ")\n",
    "\n",
    "base_model.trainable= False  \n",
    "#for layer in base_model.layers[-7:]:\n",
    "    #layer.trainable = True\n",
    "\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=[32,32,3], batch_size= batch_size))\n",
    "    model.add(layers.Resizing(224,224,interpolation='bilinear'))\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    hp_activation = hp.Choice('activation', values = ['relu', 'tanh'])\n",
    "    hp_layer_1 = hp.Int('Dense_1', min_value = 100, max_value = 2048, step = 100)\n",
    "    hp_layer_2 = hp.Int('Dense_2', min_value = 100, max_value = 2048, step = 100)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2,1e-3,1e-4,1e-5,1e-6,1e-7])\n",
    "\n",
    "    \n",
    "    model.add(layers.Dense(units=hp_layer_1, activation=hp_activation))\n",
    "    model.add(layers.Dense(units=hp_layer_2, activation=hp_activation))\n",
    "    model.add(layers.Dense(100, activation='softmax', name='output'))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=keras.losses.CategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(model_builder, \n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 72 Complete [00h 13m 43s]\n",
      "val_accuracy: 0.6614000201225281\n",
      "\n",
      "Best val_accuracy So Far: 0.7475000023841858\n",
      "Total elapsed time: 06h 48m 15s\n",
      "\n",
      "Search: Running Trial #73\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "tanh              |tanh              |activation\n",
      "1900              |1900              |Dense_1\n",
      "400               |1600              |Dense_2\n",
      "1e-05             |1e-05             |learning_rate\n",
      "50                |50                |tuner/epochs\n",
      "17                |17                |tuner/initial_epoch\n",
      "2                 |3                 |tuner/bracket\n",
      "2                 |3                 |tuner/round\n",
      "0068              |0047              |tuner/trial_id\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', \n",
    "    factor=0.7,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='max',\n",
    "    min_lr=0.00000001\n",
    ")\n",
    "\n",
    "early_stopping =callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "tuner.search(x_train,y_train, epochs = 50, validation_split=0.2, callbacks=[early_stopping,reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "# Train the model using mixup-augmented data\n",
    "history=model.fit(x_train,y_train, #train_gen\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          epochs=50,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[reduce_lr,early_stopping]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_eval=model.evaluate(x_test,y_test)\n",
    "\n",
    "\n",
    "# Get model predictions\n",
    "y_pred_probs = model.predict(x_test)  # Get probability predictions\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # Convert to class labels\n",
    "\n",
    "print(y_eval)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assume y_true and y_pred are defined. They might be one-hot encoded arrays.\n",
    "# Convert y_true to class labels if it is one-hot encoded.\n",
    "if y_test.ndim > 1:\n",
    "    y_true_class = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_true_class = y_test\n",
    "\n",
    "# Convert y_pred to class labels if it is one-hot encoded.\n",
    "if y_pred.ndim > 1:\n",
    "    y_pred_class = np.argmax(y_pred, axis=1)\n",
    "else:\n",
    "    y_pred_class = y_pred\n",
    "\n",
    "# Now compute the metrics with both arrays in the same format.\n",
    "precision_val = precision_score(y_true_class, y_pred_class, average='macro')\n",
    "recall_val = recall_score(y_true_class, y_pred_class, average='macro')\n",
    "f1_val = f1_score(y_true_class, y_pred_class, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision_val}\")\n",
    "print(f\"Recall: {recall_val}\")\n",
    "print(f\"F1 Score: {f1_val}\")\n",
    "\n",
    "# And print the full classification report.\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_class, y_pred_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
